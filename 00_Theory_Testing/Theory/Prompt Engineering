### General best practices for prompt engineering (OpenAI)
(https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api)

#General/Format
- Use the latest models
- Place instructions at the beginning: Put the instructions at the beginning of 
  the prompt and separate them from the context text with ### or “”"
- Be specific, descriptive, and detailed: Describe the desired context, outcome, 
  length, format, and style as precisely as possible.
- Articulate the desired output format with examples: Show the model what the 
  desired format looks like (“show, don't tell”), as this improves the response 
  and facilitates programmatic evaluation.
- Reduce “fluffy” and imprecise descriptions: Be concise and clear in your instructions.
- Tell them what to do, rather than just what not to do: Instead of issuing 
  prohibitions, give clear instructions for the desired action.
- Note parameters: model: higher performance models have higher latency. 
  Temperature: controls randomness of output, higher temperatures lead to more
  creative answers but more random results. 
- For complex tasks that often require deeper thinking, problem solving, or 
  the integration of external information
  
#Process: 
- Start with zero-shot, then few-shot: First try zero-shot prompting (without examples), 
  then few-shot prompting (with a few examples). If neither works, fine-tuning 
  may be an option.
  
### For more complex tasks: 

#Example: PromptWizard (LINK):
Uses a closed feedback system in which an LLM independently creates, tests
evaluates and ompimizes betetr prompts. Model self evaluates answers. 

--> Chain-of-thoughts Prompting: model is instructed to think in logical intermediate
stepts. ==> should increace accuracy. 
--> self-critique and reflexion: the model is asked if its answeris correct, 
what could be improved.
--> Few-shot with examples: Examples of correct classifications are provided in
the prompt to show the pattern. Select the most informative examples.

##
According to recent literature and OpenAI’s official recommendations for prompt 
engineering and studies like PromptWizard, successful prompts for complex tasks
should:
- be clear, structured, and example-based (“show, don’t tell”)
- use few-shot examples rather than zero-shot when tasks are more complex
- include reasoning steps or reflection to simulate human decision processes
- specify the output format explicitly to aid consistency and processing
- test different levels of instruction detail and logic structuring to find optimal reliability

As the task is more complex, zero-shot prompting is likely insufficient. 
The prompt formats to test should therefor include best-practices like chain-of-thought 
reasoning, role assignment and example-based learning (few-shot).



